{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf5f36f-5af4-483b-8577-1c2dcd0b1b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistralai\n",
      "  Downloading mistralai-1.10.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting flask\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: streamlit in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (1.50.0)\n",
      "Requirement already satisfied: requests in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (2.32.5)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
      "  Downloading eval_type_backport-0.3.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: httpx>=0.28.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai)\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
      "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions<0.60,>=0.59b0 (from mistralai)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pydantic>=2.10.3 (from mistralai)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from mistralai) (6.0.2)\n",
      "Collecting typing-inspection>=0.4.0 (from mistralai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->mistralai) (4.15.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->mistralai) (3.21.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from opentelemetry-proto==1.39.1->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai) (6.33.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests) (2025.10.5)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
      "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
      "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from mistralai)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from mistralai)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
      "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0 (from mistralai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.37.0->mistralai)\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from flask) (8.1.8)\n",
      "Collecting itsdangerous>=2.2.0 (from flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from flask) (3.0.2)\n",
      "Collecting werkzeug>=3.1.0 (from flask)\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (6.2.6)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (2.0.1)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (3.1.46)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpx>=0.28.1->mistralai) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.22.3)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.10.3->mistralai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.10.3->mistralai)\n",
      "  Downloading pydantic_core-2.41.5-cp39-cp39-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\basha\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.0)\n",
      "Downloading mistralai-1.10.0-py3-none-any.whl (460 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading eval_type_backport-0.3.1-py3-none-any.whl (6.1 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.0 MB/s  0:00:01\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Installing collected packages: werkzeug, typing-inspection, python-dotenv, pydantic-core, opentelemetry-proto, itsdangerous, invoke, googleapis-common-protos, eval-type-backport, annotated-types, pydantic, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, flask, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, mistralai\n",
      "\n",
      "   ----------------------------------------  0/18 [werkzeug]\n",
      "   ----------------------------------------  0/18 [werkzeug]\n",
      "   ------ ---------------------------------  3/18 [pydantic-core]\n",
      "   -------- -------------------------------  4/18 [opentelemetry-proto]\n",
      "   ------------- --------------------------  6/18 [invoke]\n",
      "   ------------- --------------------------  6/18 [invoke]\n",
      "   --------------- ------------------------  7/18 [googleapis-common-protos]\n",
      "   --------------- ------------------------  7/18 [googleapis-common-protos]\n",
      "   ----------------- ----------------------  8/18 [eval-type-backport]\n",
      "   ---------------------- ----------------- 10/18 [pydantic]\n",
      "   ---------------------- ----------------- 10/18 [pydantic]\n",
      "   ---------------------- ----------------- 10/18 [pydantic]\n",
      "   ---------------------- ----------------- 10/18 [pydantic]\n",
      "   -------------------------- ------------- 12/18 [opentelemetry-api]\n",
      "   ---------------------------- ----------- 13/18 [flask]\n",
      "   ------------------------- ------- 14/18 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 14/18 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 14/18 [opentelemetry-semantic-conventions]\n",
      "   --------------------------------- ------ 15/18 [opentelemetry-sdk]\n",
      "   --------------------------------- ------ 15/18 [opentelemetry-sdk]\n",
      "   --------------------------------- ------ 15/18 [opentelemetry-sdk]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ------------------------------------- -- 17/18 [mistralai]\n",
      "   ---------------------------------------- 18/18 [mistralai]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 eval-type-backport-0.3.1 flask-3.1.2 googleapis-common-protos-1.72.0 invoke-2.2.1 itsdangerous-2.2.0 mistralai-1.10.0 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-http-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 pydantic-2.12.5 pydantic-core-2.41.5 python-dotenv-1.2.1 typing-inspection-0.4.2 werkzeug-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai python-dotenv flask streamlit requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c2c2a0-60ee-4239-ba03-5845d0a41f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the key exist check ? True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # reads .env\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "print(\"Does the key exist check ?\", api_key is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4deacd7-7570-44c2-8335-04e2f077c8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I can help with a wide range of tasks, including:\n",
      "\n",
      "- **Answering Questions**: General knowledge, science, history, technology, and more.\n",
      "- **Writing Assistance**: Essays, emails, stories, reports, and creative writing.\n",
      "- **Coding Help**: Debugging, explaining code, or generating snippets in Python, JavaScript, Java, and other languages.\n",
      "- **Learning & Education**: Explaining concepts, summarizing topics, or providing study guides.\n",
      "- **Productivity & Organization**: Task management, note-taking, and planning.\n",
      "- **Entertainment**: Jokes, trivia, word games, and fun facts.\n",
      "- **Language Support**: Translation, grammar checks, and language learning tips.\n",
      "- **Recommendations**: Books, movies, music, recipes, and travel ideas.\n",
      "\n",
      "Just let me know what you need, and I‚Äôll do my best to assist! üòä\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "def mistral_chat(user_message, model=\"mistral-small-latest\", is_json=False, temperature=0.2):\n",
    "    client = Mistral(api_key=api_key)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    kwargs = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    if is_json:\n",
    "        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "    resp = client.chat.complete(**kwargs)\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "print(mistral_chat(\"hello, what can you do?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec24fa7f-199b-407f-9a33-2854e76ff0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt = \"\"\"\n",
    "You are a bank customer service bot.\n",
    "\n",
    "Your task is to assess customer intent and categorize the customer inquiry\n",
    "after <<<>>> into one of the following predefined categories:\n",
    "\n",
    "card arrival\n",
    "change pin\n",
    "exchange rate\n",
    "country support\n",
    "cancel transfer\n",
    "charge dispute\n",
    "\n",
    "If the text doesn't fit into any of the above categories,\n",
    "classify it as:\n",
    "customer service\n",
    "\n",
    "You will only respond with the predefined category.\n",
    "Do not provide explanations or notes.\n",
    "\n",
    "###\n",
    "Here are some examples:\n",
    "\n",
    "Inquiry: How do I know if I will get my card, or if it is lost?\n",
    "Category: card arrival\n",
    "\n",
    "Inquiry: I am planning an international trip and would like to inquire about the current exchange rates.\n",
    "Category: exchange rate\n",
    "\n",
    "Inquiry: What countries are getting support? I will be traveling and living abroad.\n",
    "Category: country support\n",
    "\n",
    "Inquiry: Can I get help starting my computer? I am having difficulty starting it.\n",
    "Category: customer service\n",
    "\n",
    "###\n",
    "<<<\n",
    "Inquiry: {inquiry}\n",
    ">>>\n",
    "Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "901d8f24-04a5-4961-a484-c66c5b73cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a bank customer service bot.\n",
      "\n",
      "Your task is to assess customer intent and categorize the customer inquiry\n",
      "after <<<>>> into one of the following predefined categories:\n",
      "\n",
      "card arrival\n",
      "change pin\n",
      "exchange rate\n",
      "country support\n",
      "cancel transfer\n",
      "charge dispute\n",
      "\n",
      "If the text doesn't fit into any of the above categories,\n",
      "classify it as:\n",
      "customer service\n",
      "\n",
      "You will only respond with the predefined category.\n",
      "Do not provide explanations or notes.\n",
      "\n",
      "###\n",
      "Here are some examples:\n",
      "\n",
      "Inquiry: How do I know if I will get my card, or if it is lost?\n",
      "Category: card arrival\n",
      "\n",
      "Inquiry: I am planning an international trip and would like to inquire about the current exchange rates.\n",
      "Category: exchange rate\n",
      "\n",
      "Inquiry: What countries are getting support? I will be traveling and living abroad.\n",
      "Category: country support\n",
      "\n",
      "Inquiry: Can I get help starting my computer? I am having difficulty starting it.\n",
      "Category: customer service\n",
      "\n",
      "###\n",
      "<<<\n",
      "Inquiry: {inquiry}\n",
      ">>>\n",
      "Category:\n"
     ]
    }
   ],
   "source": [
    "# fixed_prompt = mistral_chat(\n",
    "#     \"Please correct the spelling and grammar of this prompt and return the same prompt with fixes:\\n\"\n",
    "#     + classification_prompt\n",
    "# )\n",
    "# print(fixed_prompt)\n",
    "\n",
    "rewrite_instruction = \"\"\"\n",
    "You are a prompt editor.\n",
    "\n",
    "Task: Fix spelling/grammar ONLY. Do NOT change meaning.\n",
    "Return ONLY the corrected prompt text.\n",
    "Do NOT add introductions, bullet points, markdown, or explanations.\n",
    "Do NOT include ‚ÄúHere is‚Ä¶‚Äù, ‚ÄúKey corrections‚Ä¶‚Äù, or any extra text.\n",
    "Output must start with the first character of the prompt and end with the last character of the prompt.\n",
    "\n",
    "PROMPT TO FIX:\n",
    "<<<\n",
    "{prompt}\n",
    ">>>\n",
    "\"\"\".strip()\n",
    "\n",
    "fixed_prompt = mistral_chat(rewrite_instruction.format(prompt=classification_prompt), temperature=0.0)\n",
    "print(fixed_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4403e24a-0a50-4681-b53b-49c30c0c7e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: country support\n"
     ]
    }
   ],
   "source": [
    "inquiry_text = \"I am inquiring about the availability of your cards in the EU\"\n",
    "category = mistral_chat(fixed_prompt.format(inquiry=inquiry_text))\n",
    "print(\"Predicted category:\", category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc1936f2-82a5-49f6-94b1-5da8479d8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_notes = \"\"\"\n",
    "A 60-year-old male patient, Mr. Johnson, presented with symptoms\n",
    "of increased thirst, frequent urination, fatigue, and unexplained\n",
    "weight loss. Upon evaluation, he was diagnosed with diabetes,\n",
    "confirmed by elevated blood sugar levels. Mr. Johnson's weight\n",
    "is 210 lbs. He has been prescribed Metformin to be taken twice daily\n",
    "with meals. It was noted during the consultation that the patient is\n",
    "a current smoker.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38272c6b-0b37-474d-9865-47f6e11f21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_prompt = f\"\"\"\n",
    "Extract information from the following medical notes:\n",
    "\n",
    "{medical_notes}\n",
    "\n",
    "Return a JSON object that matches this schema exactly:\n",
    "{{\n",
    "  \"age\": {{\"type\": \"integer\"}},\n",
    "  \"gender\": {{\"type\": \"string\", \"enum\": [\"male\", \"female\", \"other\"]}},\n",
    "  \"diagnosis\": {{\"type\": \"string\", \"enum\": [\"migraine\", \"diabetes\", \"arthritis\", \"acne\"]}},\n",
    "  \"weight\": {{\"type\": \"integer\"}},\n",
    "  \"smoking\": {{\"type\": \"string\", \"enum\": [\"yes\", \"no\"]}}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "575cd2a5-b70c-43f1-8a67-4a271efc1a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"age\": 60,\n",
      "  \"gender\": \"male\",\n",
      "  \"diagnosis\": \"diabetes\",\n",
      "  \"weight\": 210,\n",
      "  \"smoking\": \"yes\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_json = mistral_chat(extract_prompt, is_json=True)\n",
    "print(response_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82db067-e211-460f-b565-c9af63c58dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear mortgage lender,\n",
    "What's your 30-year fixed-rate APR, how is it compared to the 15-year\n",
    "fixed rate?\n",
    "Regards,\n",
    "Anna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24fc9f61-8b8b-4ae5-a813-732d114ae129",
   "metadata": {},
   "outputs": [],
   "source": [
    "personalization_prompt = f\"\"\"\n",
    "You are a mortgage lender customer service bot. Your task is to create\n",
    "personalized email responses to address customer questions.\n",
    "\n",
    "Answer the customer's inquiry using the provided facts below.\n",
    "Ensure your response is clear, concise, and directly addresses the question.\n",
    "Address the customer in a friendly and professional manner.\n",
    "The LAST line of your response MUST be exactly:\n",
    "Lender Customer Support\n",
    "\n",
    "Do not omit it.\n",
    "\n",
    "Return ONLY the email body.\n",
    "Do NOT include a subject line.\n",
    "Do NOT use markdown formatting.\n",
    "Do NOT use bold text.\n",
    "Output plain text only.\n",
    "\n",
    "# Facts\n",
    "0-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
    "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
    "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
    "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
    "7-year ARM: interest rate 7.011%, APR 7.660%\n",
    "5-year ARM: interest rate 6.880%, APR 7.754%\n",
    "3-year ARM: interest rate 6.125%, APR 7.204%\n",
    "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
    "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
    "\n",
    "# Email\n",
    "{email}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d08a317-678f-4aca-b88a-0d95df2fa508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Anna,\n",
      "\n",
      "Thank you for reaching out! Our current 30-year fixed-rate APR is 6.484%. In comparison, the 15-year fixed-rate APR is 5.848%. As you can see, the 15-year fixed-rate option has a lower APR, which typically means lower overall interest costs over the life of the loan, though the monthly payments may be higher due to the shorter term.\n",
      "\n",
      "Please let us know if you have any other questions or need further assistance.\n",
      "\n",
      "Lender Customer Support\n"
     ]
    }
   ],
   "source": [
    "reply = mistral_chat(personalization_prompt)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a40d657f-7492-4034-b920-fa297aa94f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsletter = \"\"\"\n",
    "European AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft. \n",
    "\n",
    "What‚Äôs new: Mistral AI introduced two closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and it agreed to distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for free here and to use on its La Plateforme and via custom deployments.\n",
    "\n",
    "Model specs: The new models‚Äô parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context. \n",
    "\n",
    "Mistral Large achieved 81.2 percent on the MMLU benchmark, outperforming Anthropic‚Äôs Claude 2, Google‚Äôs Gemini Pro, and Meta‚Äôs Llama 2 70B, though falling short of GPT-4. Mistral Small, which is optimized for latency and cost, achieved 72.2 percent on MMLU.\n",
    "Both models are fluent in French, German, Spanish, and Italian. They‚Äôre trained for function calling and JSON-format output.\n",
    "Microsoft‚Äôs investment in Mistral AI is significant but tiny compared to its $13 billion stake in OpenAI and Google and Amazon‚Äôs investments in Anthropic, which amount to $2 billion and $4 billion respectively.\n",
    "Mistral AI and Microsoft will collaborate to train bespoke models for customers including European governments.\n",
    "Behind the news: Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France‚Äôs representatives in the European Commission argued on Mistral‚Äôs behalf to loosen the European Union‚Äôs AI Act oversight on powerful AI models. \n",
    "\n",
    "Yes, but: Mistral AI‚Äôs partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already was investigating Microsoft‚Äôs agreement with OpenAI for potential breaches of antitrust law, plans to investigate the new partnership as well. Members of President Emmanuel Macron‚Äôs Renaissance party criticized the deal‚Äôs potential to give a U.S. company access to European users‚Äô data. However, other French lawmakers support the relationship.\n",
    "\n",
    "Why it matters: The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that‚Äôs tailored to Europe‚Äôs unique regulatory environment.\n",
    "\n",
    "We‚Äôre thinking: Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "156b29b9-c84f-4f13-84f7-b31234244c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Summary\n",
      "\n",
      "Mistral AI, a European AI startup, has unveiled two new large language models (LLMs), Mistral Large and Mistral Small, joining its earlier Mistral Medium model. The company has formed a strategic partnership with Microsoft, which includes a $16.3 million investment and an agreement to distribute Mistral Large on Azure. The models are available for free trials and custom deployments, with both models supporting multilingual capabilities (French, German, Spanish, Italian) and function calling.\n",
      "\n",
      "Mistral Large outperformed competitors like Claude 2, Gemini Pro, and Llama 2 70B on the MMLU benchmark (81.2%) but fell short of GPT-4. Mistral Small, optimized for latency and cost, achieved 72.2% on the same benchmark. The partnership with Microsoft provides Mistral AI with access to Azure‚Äôs computing infrastructure and global distribution, while Microsoft gains a foothold in the European AI market.\n",
      "\n",
      "The alliance has sparked regulatory scrutiny, with the European Commission investigating potential antitrust concerns, particularly given Microsoft‚Äôs existing deal with OpenAI. Some French lawmakers criticize the deal for potentially giving U.S. companies access to European user data, while others support the collaboration. Mistral AI, founded by ex-Google and Meta engineers, is positioned as a European competitor to U.S. AI leaders like OpenAI.\n",
      "\n",
      "---\n",
      "\n",
      "# Interesting Questions\n",
      "\n",
      "### Q1: How does Mistral AI‚Äôs performance compare to other leading AI models, and what are the implications of its benchmark results?\n",
      "**A:** Mistral AI‚Äôs Mistral Large model achieved an 81.2% score on the MMLU benchmark, outperforming competitors like Anthropic‚Äôs Claude 2, Google‚Äôs Gemini Pro, and Meta‚Äôs Llama 2 70B. However, it still trails behind OpenAI‚Äôs GPT-4. Mistral Small, optimized for efficiency, scored 72.2% on the same benchmark. These results suggest that Mistral AI is competitive but not yet at the cutting edge of AI performance. The implications are that Mistral AI is positioning itself as a strong European alternative to U.S.-based models, which could be significant for regulatory and market dynamics in Europe. <**Mistral AI is competitive but not yet at the cutting edge of AI performance.**>\n",
      "\n",
      "### Q2: What are the potential risks and benefits of Mistral AI‚Äôs partnership with Microsoft for European regulators and users?\n",
      "**A:** The partnership provides Mistral AI with access to Microsoft‚Äôs Azure infrastructure, which is crucial for training and deploying large models. For Microsoft, it offers a stronger foothold in the European AI market. However, the deal has raised concerns among European regulators, particularly the European Commission, which is investigating potential antitrust violations. Some lawmakers worry that the partnership could give U.S. companies access to European user data, while others see it as a necessary collaboration to advance AI in Europe. The benefits include faster AI development and deployment, while the risks involve data sovereignty and market dominance by U.S. tech giants. <**The partnership offers significant benefits for AI development but raises concerns about data sovereignty and antitrust compliance.**>\n",
      "\n",
      "### Q3: How does Mistral AI‚Äôs rapid growth and success reflect broader trends in the AI industry?\n",
      "**A:** Mistral AI‚Äôs quick rise‚Äîfounded in early 2023 and already competing with established players‚Äîhighlights the accelerating pace of AI innovation. Its success also underscores the importance of partnerships with hyperscalers like Microsoft, which provide the computational resources and distribution channels needed to scale AI models. The fact that Mistral AI is a European startup gaining traction against U.S. giants reflects a growing push for regional AI champions, particularly in Europe, where regulatory and geopolitical factors are driving investment in homegrown AI. This trend suggests that the AI landscape is becoming more fragmented, with regional players emerging alongside global leaders. <**Mistral AI‚Äôs success reflects the accelerating pace of AI innovation and the growing importance of regional AI champions.**>\n",
      "\n",
      "---\n",
      "\n",
      "# Analysis Report\n",
      "\n",
      "```markdown\n",
      "# Mistral AI and Microsoft Partnership: A Strategic Shift in the European AI Landscape\n",
      "\n",
      "## Overview\n",
      "Mistral AI, a French AI startup, has unveiled two new large language models (LLMs), Mistral Large and Mistral Small, and formed a strategic partnership with Microsoft. The collaboration includes a $16.3 million investment from Microsoft and an agreement to distribute Mistral Large on Azure. The models are available for free trials and custom deployments, with both supporting multilingual capabilities and function calling.\n",
      "\n",
      "## Key Developments\n",
      "- **Model Performance:** Mistral Large achieved an 81.2% score on the MMLU benchmark, outperforming competitors like Claude 2, Gemini Pro, and Llama 2 70B but trailing GPT-4. Mistral Small, optimized for efficiency, scored 72.2%.\n",
      "- **Microsoft Partnership:** The deal provides Mistral AI with access to Azure‚Äôs computing infrastructure and global distribution, while Microsoft gains a stronger foothold in the European AI market.\n",
      "- **Regulatory Scrutiny:** The European Commission is investigating the partnership for potential antitrust violations, with concerns about U.S. companies accessing European user data.\n",
      "\n",
      "## Implications\n",
      "1. **Competitive Positioning:** Mistral AI is emerging as a strong European alternative to U.S.-based AI leaders, positioning itself as a regional champion in the global AI race.\n",
      "2. **Regulatory Challenges:** The partnership highlights tensions between fostering AI innovation and protecting data sovereignty, with European regulators balancing the need for collaboration with concerns about market dominance.\n",
      "3. **Industry Trends:** Mistral AI‚Äôs rapid growth reflects broader trends in AI, including the importance of partnerships with hyperscalers and the rise of regional AI champions.\n",
      "\n",
      "## Conclusion\n",
      "The Mistral AI-Microsoft partnership is a significant development in the European AI landscape, offering both opportunities and challenges. While it accelerates AI innovation and deployment, it also raises regulatory and geopolitical concerns. The success of Mistral AI underscores the growing importance of regional players in the global AI ecosystem.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "summ_prompt = f\"\"\"\n",
    "You are a commentator. Your task is to write a report on a newsletter. \n",
    "When presented with the newsletter, come up with interesting questions to ask,\n",
    "and answer each question. \n",
    "Afterward, combine all the information and write a report in the markdown\n",
    "format. \n",
    "\n",
    "# Newsletter: \n",
    "{newsletter}\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes \n",
    "presented in the newsletter.\n",
    "\n",
    "## Interesting Questions: \n",
    "Generate three distinct and thought-provoking questions that can be \n",
    "asked about the content of the newsletter. For each question:\n",
    "- After \"Q: \", describe the problem \n",
    "- After \"A: \", provide a detailed explanation of the problem addressed \n",
    "in the question.- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a analysis report\n",
    "Using the summary and the answers to the interesting questions, \n",
    "create a comprehensive report in Markdown format. \n",
    "\"\"\"\n",
    "\n",
    "report = mistral_chat(summ_prompt)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a0978d2-2be2-490d-afee-5a0463a19069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'intent': 'order_status', 'reply': '**Support Reply:**\\nThank you for reaching out! I‚Äôd be happy to help track your order. Could you please share your order ID or the email associated with the purchase? This will allow me to check the latest status for you.\\n\\n*(If the user provides the details, proceed with tracking. If not, ask for one more piece of info, e.g., \"Was this order placed online or in-store?\")*\\n\\nIf the issue persists, I can escalate this to a human agent‚Äîwould you like me to do that? If so, please provide your preferred contact method (email/phone).'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.post(\n",
    "    \"http://127.0.0.1:5000/chat\",\n",
    "    json={\"message\": \"My order is late, where is it?\"}\n",
    ")\n",
    "\n",
    "print(resp.status_code)\n",
    "print(resp.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f4003-e3fb-4d83-a44e-472e338f05e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
